---
title: "Playtime with taxi data"
author: "John Little"
format: html
---

## Purpose

The purpose of this notebook is to give you access to a toy dataset where you can practice {duckplyr} queries and exploratory data analysis.

**Goal: play with the data.**

### Disclaimer

While the data could be accessed through {duckplyr} via HTTPS – similar to the demonstration in `02_ingest_CSV_and_SQL.qmd`{=} – I found the HTTPS data transfer was unreliable and caused numerous hiccups, due to network lag. This problem could have been transitory, occurring during my notebook development. Rather than HTTPS transfer, downloading the data may be more reliable for a workshop setting. However, downloading \~ 0.6GB of data may be time-consuming and dependent on your network throughput. Since this workshop is on campus, and in-person, there will be no network lag. Therefore, bottlenecks will become your available local disk space on your Local File system and your available RAM. In my testing, this script ran fine on a computer with 4, 16, or 64 GB of RAM. It was painfully slow, but functional, on an RStudio container with 2 GB of RAM.

## Taxi data

Pedro Holanda [published a DuckDB blog post](https://duckdb.org/2024/10/16/driving-csv-performance-benchmarking-duckdb-with-the-nyc-taxi-dataset.html) about benchmarking DuckDB with NYC Taxi data in CSV format. I've adapted Holanda's code to fit this {duckplyr} example. This notebook will download \~0.6 GB of selective taxi data and demonstrate use of basic {duckplyr} functions for data exploration. Rather than gather the CSV format, we will use the readily available parquet format.

> In 2022, the provider of taxi data decided to distribute the dataset as a series of Parquet files instead of CSV files. Performance-wise, this is a wise choice, as Parquet files are much smaller than CSV files, and their native columnar format allows for fast execution directly on them.

```{r}
library(conflicted)
library(duckplyr)
library(fs)
library(nycflights13)

conflict_prefer("filter", "dplyr", quiet = TRUE)
```

## Data download and prep

The data are described in greater detail at the [NYC trip and Limmosine Commision open data page](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page).

Based on the website above we want a vector of URLs to parquet files, and a vector of destination file paths for downloading to my local LFS.

```{r}
#| label: pull-from-url-NOT

year_tx <- 2020:2024
month_tx <- rep(stringr::str_pad(1:3, width = 2, pad = "0"), 4)
base_url_tx <- "https://d37ci6vzurychx.cloudfront.net/trip-data/" #yellow_tripdata_2024-02.parquet"
record_type <- "yellow_tripdata_"
files_tx <- paste0(record_type, year_tx, "-", month_tx, ".parquet")
files_tx

taxis_urls <- tibble(filename = files_tx, base_url = base_url_tx) |> 
  arrange(filename) |> 
  mutate(url = paste0(base_url, filename)) %>% 
  mutate(destfile = paste0("data/taxis/", filename))


taxis_urls
```

```{r}
fs::dir_create("data/taxis")

purrr::walk2(taxis_urls$url, taxis_urls$destfile, 
             download.file, 
             method = "libcurl",  
             mode = "wb") 
```

How much data did we just download?

```{r}
#| label: get-filenames

my_files <- fs::dir_ls("data/taxis", glob = "*.parquet")
my_files %>% fs::file_info()

fs::dir_info("data/taxis") %>% 
  summarise(sum(size))
```

## Import Parquet files

```{r}
#| label: ingest-parquet-file

taxis_duckdb <-  my_files %>% 
  read_parquet_duckdb()
```

```{r}
taxis_duckdb |> 
  explain()
```

How many rows of data?

```{r}
taxis_duckdb |> 
  count() 
```

Take a quick look at the top rows of the duckdb data frame

```{r}
taxis_duckdb |> 
  head() |> 
  glimpse()
```

A basic query

```{r}
taxis_duckdb |> 
  mutate(year = lubridate::year(tpep_dropoff_datetime)) |>
  summarise(avg_fare = mean(fare_amount, na.rm = TRUE), 
            avg_distance = mean(trip_distance, na.rm = TRUE),
            .by = year) |>
  mutate(avg_fare_pretty = scales::dollar(avg_fare),
         .after = avg_fare) |> 
  mutate(avgt_cost_per_mile = avg_fare / avg_distance) |> 
  arrange(-avg_fare)
```

## Your Turn

Add new code chunks.

Come up with new and novel data explorations.

## File clean-up/delete

The following code chunk is set to **not run** (i.e. `eval: false`{=}). However, individual code chunks can be run, by clicking the green execute arrows. This means, *run-all,* will not run this code, but I can manually override. Doing so will clean-up, i.e. delete, all CSV files and all downloaded parquet files.

```{r}
#| label: clean-up files
#| eval: false

fs::dir_delete("data/taxis")
csv_datafiles <- fs::dir_ls("data", glob = "*.csv")
# csv_datafiles
fs::file_delete(csv_datafiles)
```
